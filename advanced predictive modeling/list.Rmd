---
title: "Modelagem Preditiva Avançada"
subtitle: "Lista 1 - Diego Silva"
output:
  html_document:
    df_print: paged
---
<br />


# Exercício 1

> Apresente a análise da Atividade Integradora do trimestre passado usando o tidymodels. Não é necessário reproduzir todos os passos e ajustar todos os modelos, considere ajustar pelo menos 2 modelos preditivos fazendo a escolha de hiperparâmetros para avaliar o poder preditivo e decidir qual é o melhor modelo de acordo com uma métrica.
> 
> <br > 
>
> **Com objetivo de praticar a aplicação e avaliação dos modelos lecionados durante as aulas ministradas até o momento, iremos utilizar:**
>
> * *LASSO*
> * *KNN*
> * *Regressão Logística*;
> * *Árvore de Decisão*;
> * *Random Forest*;
> * *XGBoost*;

\n
\n

```{r, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE, warning = FALSE)
```

```{r, cache=TRUE, echo=FALSE, results = "hide"}
library(tidymodels)
library(tidyverse)
library(gapminder)
library(rsample)
library(doParallel)
library(yardstick)
library(readr)
library(skimr)
library(factoextra)
options(tidymodels.dark = TRUE)
```


## Lendo o conjunto de dados

```{r, reading-data, results="hide"}

df <- read_csv('data-treated.csv')

df$J1 <- as.factor(df$J1)

set.seed(123)
split <- initial_split(df, prop = .8, strata = "J1")

train <- training(split)
test <- testing(split)
```

## Criando os folds para validação cruzada que será utilizada posteriormente nos modelos

```{r, creating-cv-folds}

cv_folds <- vfold_cv(train, v = 2)

cv_folds
```

## Criando tabela para armazenar os resultados da análise

```{r, creating-results-tibble}
results <- tibble(model = NA_character_, threshold = NA_integer_,  accuracy = NA_integer_, fbeta = NA_integer_, recall = NA, precision = NA_integer_)
```

## Definindo função para calculo das métricas para cada valor de corte 

```{r, defining-gpft}
cortes <- seq(0.01, 0.99, 0.01)

get_predictions_from_threshold <- function(threshold, df_gpft, model) {
  results <- df_gpft %>% 
    mutate(threshold_preds = ifelse(df_gpft$.pred_1 >= threshold, 1, 0)) %>% 
    mutate(observed = as.numeric(as.character(observed)), threshold_preds = as.numeric(as.character(threshold_preds)))
           
 tibble(
   model = model,
   fbeta = Metrics::fbeta_score(results$observed, results$threshold_preds),
   acc = Metrics::accuracy(results$observed, results$threshold_preds),
   precision = Metrics::precision(results$observed, results$threshold_preds),
   recall = Metrics::recall(results$observed, results$threshold_preds),
   threshold = threshold
   )
}
```

## Aplicação dos modelos

**Disclaimer*: A fins de prática, serão realizadas a receita e sua preparação em todas as etapas.*

<br />

### LASSO

* Preparando a receita para o lasso

```{r, preparing-lasso-recipe}
lasso_recipe <- recipe(J1 ~ ., data = train)

lasso_prep <- prep(lasso_recipe)

lasso_juiced <- bake(lasso_prep, new_data = NULL)

lasso_baked <- bake(lasso_prep, new_data = test)
```


* Definição do Modelo

```{r, defining_lasso}
lasso <- logistic_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet") %>% 
  set_mode("classification")
```

* Visualizando a especificação 

```{r, checking_lasso_model}
lasso
```

* Realizando o tunning 

```{r, lasso_tuning}
tune_lasso <- tune_grid(
  lasso,
  lasso_recipe,
  resamples = cv_folds,
  grid = 1,
  metrics = metric_set(roc_auc, yardstick::accuracy, yardstick::specificity, yardstick::sensitivity, yardstick::f_meas)
  )
```

* Visualizando as métricas do tune_grid em relação a regularização:

```{r}
autoplot(tune_lasso)
```

* Selecionando a quantidade de regularização pelos valores obtidos da curva roc e f1:

```{r}
best_lasso <- tune_lasso %>% 
  select_best("roc_auc", "f_meas") 
```

* Ajustando o modelo final:

```{r}

fitted_lasso <- finalize_model(lasso, parameters = best_lasso) %>% 
  fit(J1 ~ ., data=lasso_juiced)
```

> Obtendo os resultados e salvando-os para cada valor de corte 

```{r}
lasso_results <- fitted_lasso %>% 
  predict(new_data = lasso_baked, type = "prob") %>%
  mutate(observed = lasso_baked$J1, model = "lasso") 

results <- results %>% 
  bind_rows(map(cortes, get_predictions_from_threshold, df_gpft = lasso_results, model = "lasso")) %>% 
  arrange(desc(fbeta))
```

### Regressão logística

> Preparando a receita para a aplicação do modelo logístico

```{r, lr-recipe}

lr_recipe <- recipe(J1 ~ ., train) %>% # define a receita, com a variavel resposta e os dados de treinamento
  step_normalize(all_numeric())

lr_prep <- prep(lr_recipe)# prepara a receita definida acima

lr_juiced <- juice(lr_prep) # obtem os dados de treinamento processados

baked_test <- bake(lr_prep, new_data = test) # obtem os dados de teste processados

```


```{r, evaluating-lr}
lr_fit <- logistic_reg() %>% 
  set_engine("glm") %>% 
  fit(J1 ~ ., lr_juiced) %>% 
  set_mode("classification")


lr_results <- lr_fit %>% 
  predict(new_data = baked_test, type = "prob") %>% 
  mutate(observed = test$J1, modelo = "logistic regression") 

results <- results %>% 
  bind_rows(map(cortes, get_predictions_from_threshold, df_gpft = lr_results, model = 'lr'))
```

<br />

### Árvore de Decisão

<br /> 

> Para a Árvore de Decisão, iremos realizar o *tuning* dos parâmetros para avaliarmos o seu poder preditivo e aplicaremos validação cruzada para estimarmos o erro de classificação.

```{r, tree-recipe}
tree_recipe <- recipe(J1 ~ ., train) %>% 
  step_normalize(all_numeric())

tree_prep <- prep(tree_recipe)

tree_juiced <- bake(tree_prep, new_data = NULL)

tree_baked <- bake(tree_prep, new_data = test)

```

> Após a preparação da receita, iremos criar e ajustar a árvore de decisão tunada. 

<br />

> Adicionando paralelismo

```{r, adding-do-parallel}
doParallel::registerDoParallel(cores = 4)
```

<br />

> Especificando a árvore de decisão tunada

```{r, defining-decision-tree}

tree <- decision_tree(
  tree_depth = tune(), 
  cost_complexity = tune()) %>%
  set_engine("rpart") %>% 
  set_mode("classification") 

tree
```

> Tunando a árvore

```{r, tunning-tree}

tune_tree <- tune_grid(
  tree,
  tree_recipe,
  resamples = cv_folds,
  grid = 1,
  metrics = metric_set(roc_auc, yardstick::accuracy, yardstick::specificity, yardstick::sensitivity, yardstick::f_meas)
  )

tune_tree$.notes
```

> Analisando o gráfico contendo as métricas e os valores obtidos para cada um dos valores assumidos para os hiperparâmetros tunados:

```{r, autoplot-tree}
autoplot(tune_tree)
```

> Coletando as métricas

```{r, collecting-tree-metrics}

tune_tree %>% 
  collect_metrics() %>% 
  head(5)
```


> Selecionando os melhores valores para os hiperparâmetros pela curva roc e salvando-os para ajustarmos o modelo final com os valores encontrados.

> Visualizando os valores de hiperparâmetro que nos dá os melhores valores de roc_auc e specificity

```{r, visualizing-best-tree}
best_tree <- tune_tree %>% 
  select_best("roc_auc", "f_meas")
```

> Finalizando o modelo e obtendo as predictions para cada valor de corte. Adicionaremos também a tibble resultante a tibble dos nossos resultados.


```{r, finalizng-tree}
fit_tree <- finalize_model(tree, parameters = best_tree) %>% 
  fit(J1 ~ ., data=train)
 
tree_preds <- fit_tree %>% 
  predict(new_data = test, type = "prob") %>% 
  mutate(observed = test$J1)

results <- results %>% 
  bind_rows(map(cortes, get_predictions_from_threshold, df_gpft = tree_preds, model = 'tree'))
```

### Floresta Aleatória

> Assim como na Árvore de Decisão, também realizaremos o tunning dos hiperparâmetros e validação cruzada para estimarmos o erro de classificação.

> Começaremos pela receita:

```{r}
rf_recipe <- recipe(J1 ~ ., data = train)

rf_prep <- prep(rf_recipe)

rf_baked_train <- bake(rf_prep, new_data = NULL)

rf_baked_test <- bake(rf_prep, new_data = test)
```

> Seguiremos agora para a especificação do modelo:

```{r}
dials::trees()

rf <- rand_forest(
  mtry = tune(),
  trees = 10,
  min_n = tune()
) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

rf
```

> Tunando o modelo

```{r message=TRUE}

tune_rf <- tune_grid(
  rf,
  rf_recipe,
  resamples = cv_folds,
  grid = 1,
  control = control_grid(verbose = TRUE)
)

tune_rf
```

> Selecionando o melhor modelo pela área abaixo da curva e f1

```{r}
best_rf <- tune_rf %>% 
  select_best("roc_auc", "f_meas")
```


> Finalizando o modelo com os hiperparametros obtidos

```{r}
fit_rf <- finalize_model(rf, parameters = best_rf) %>% 
  fit(J1 ~ ., data = train)

```

> Calculando as métricas de interesse para cada valor de corte

```{r}

rf_preds <-  fit_rf %>%
  predict(new_data = test, type="prob") %>% 
  mutate(observed = rf_baked_test$J1, model = "rf") 

results <- results %>% 
  bind_rows(map(cortes, get_predictions_from_threshold, df_gpft = rf_preds, model = 'rf'))
```


```{r}
stopImplicitCluster()
```


```{r}
results
```


# Exercício 2

> Considere um modelo linear com todas as variáveis preditoras e um modelo linear com 5 componentes principais para previsão do valor mediano dos imóveis do banco de dados Boston (acesse com
MASS::Boston ). Para avaliar o desempenho dos modelos, faça uma divisão dos dados de 80% para treinamento e o restante para teste. Indique qual métrica está sendo utilizada para a comparação dos modelos e defina o melhor modelo preditivo. Dicas:
Note que queremos predizer o valor mediano dos imóveis, não médio. O que isso muda em nossa avaliação?
Para comparar o modelo sem PCA e com PCA, você pode usar tune() no número de componentes e, na tune_grid() adicionar os valores 0 e 4. O valor zero fará com que o PCA não seja ajustado, enquanto que o valor 4 considerará 4 componentes principais.


```{r, echo=FALSE}
library(MASS)
```


> Para a previsão do valor mediano dos imovéis, iremos minimizar o MAE (*Mean Absolute Error*), uma vez que isto indicaria uma boa previsibilidade de valores medianos. Em relação aos modelos que serão utilizados, segue a lista abaixo:
>
> * Regressão Linear
> * LASSO
> * Ridge
> * Árvore de Decisão
> * Floresta Aleatória
> * XGBoost

> Carregaremos o dataset e veremos a distribuição dos valores obtidos em cada uma das colunas

```{r}
df <- MASS::Boston
summary(df)

```

> Separando os dados em treino e teste, e então criando os *cross-validation folds*

```{r}

set.seed(1232)

splits <- initial_split(df, prop = .8, strata = "medv")

train <- training(splits)
test <- testing(splits)

cv_folds <- vfold_cv(train, v = 2)
```

> Definindo tibble para armazenar os resultados obtidos por cada modelo em relação a métrica de interesse:

```{r}
results <- tibble(model = NA, mae = NA_integer_)
```

## Regressão Linear

> Preparando a receita:

```{r}
# escolher colunas a serem aplicadas pca
lm_recipe <- recipe(medv ~ ., data = train) %>% 
  step_normalize(all_numeric(), -all_outcomes()) %>% 
  step_pca(c(where(is.numeric), -age, -all_outcomes()), num_comp = 4)

lm_prep <- prep(lm_recipe)

train_lm <- bake(lm_prep, new_data = NULL)

test_lm <- bake(lm_prep, new_data = test)
  
```


* Estimando o erro de *MAE* do modelo linear com validação cruzada:

```{r}
lm_fit <- linear_reg() %>% 
  set_engine("lm")

lm_fitted <- lm_fit %>% 
  fit(medv ~ ., data = train_lm)
  
```

* Obtendo as previsões e o *MAE* do modelo linear ajustado e salvando-as no tibble de resultados:

```{r}

results <- results %>% 
  bind_rows(
    tibble(
      model = "lm", 
      mae = Metrics::mae(test_lm$medv, predict(lm_fitted, test_lm)$.pred)
      )
    )
```
## LASSO

* Preparando a receita para o modelo LASSO:

```{r}

lasso_recipe <- recipe(medv ~ ., data = train) %>% 
  step_normalize(all_numeric(), -all_outcomes())  %>% 
  step_pca(c(where(is.numeric), -age, -all_outcomes()), num_comp = tune())

lasso_train <- bake(lasso_prep, new_data = NULL)

lasso_test <- bake(lasso_prep, new_data = test)

```

Definição do modelo e do workflow:

```{r}
lasso_fit <- linear_reg(
  penalty = tune(),
  mixture = 1
) %>% 
  set_engine("glmnet") %>% 
  set_mode("regression") 

lasso_workflow <- workflow() %>% 
  add_model(lasso_fit) %>% 
  add_recipe(lasso_recipe)

```

Tunando a regularização utilizando validação cruzada:

```{r}

lasso_grid <- dials::grid_regular(
  penalty(),
  num_comp(c(0L, 4L)), 
  levels = 2 
)

lasso_tuned <- lasso_workflow %>% 
  tune_grid(
    cv_folds,
    grid = lasso_grid,
    control_grid = control_grid(verbose = TRUE, save_pred = TRUE),
    metrics = metric_set(yardstick::mae)
  )

```

* Visualizando o gráfico com os resultados:

```{r}
autoplot(lasso_tuned)
```

* Selecionando a regularização pelo melhor valor obtido de *MAE*:

```{r}
best_lasso <- lasso_tuned %>% 
  select_best("mae")

best_lasso
```

* Finalizando o modelo lasso:

```{r}
final_lasso <- finalize_model(lasso_fit, parameters = best_lasso) %>% 
  fit(medv ~ ., data = train)
```


* Estimando o *MAE* do modelo lasso ajustado e salvado-o em nosso *tibble* de resultados:


```{r}
results <- results %>% 
  bind_rows(tibble(model = "lasso", mae = Metrics::mae(test$medv, predict(final_lasso, test)$.pred)))

```

## Ridge

Preparando a receita:

```{r}

ridge_recipe <- recipe(medv ~ ., data = train) %>% 
  step_normalize(all_numeric(), -all_outcomes()) %>% 
  step_pca(c(where(is.numeric), -age, -all_outcomes()), num_comp = tune())

```

Definindo o modelo e do workflow:

```{r}

ridge_fit <- linear_reg(
  penalty = tune(),
  mixture = 0
  ) %>% 
  set_engine("glmnet") %>% 
  set_mode("regression")


ridge_workflow <- workflow() %>% 
  add_model(ridge_fit) %>% 
  add_recipe(ridge_recipe)

```

Realizando validação cruzada:

```{r}

ridge_grid <- dials::grid_regular(
  penalty(),
  num_comp(c(0L, 5L)),
  levels = 2
)


ridge_tuned <- ridge_workflow %>% 
  tune_grid(
    resamples = cv_folds,
    grid = ridge_grid,
    metrics = metric_set(yardstick::mae),
    control = control_grid(verbose = TRUE, save_pred = TRUE)
  )
```

Visualizando o gráfico do *MAE* em relação a regularização no modelo ridge:

```{r}
autoplot(ridge_tuned)
```

Selecionando os valores de componentes com melhor *MAE* e salvando o *MAE* do modelo ridge obtido:

```{r}
best_ridge <- ridge_tuned %>% 
  select_best()
  
final_ridge <- finalize_model(ridge_fit, parameters = best_ridge) %>% 
  fit(medv ~ ., data = train)

results <- results %>% 
  bind_rows(
    tibble(
      model = "ridge",
      mae = Metrics::mae(test$medv, predict(final_ridge, test)$.pred)
    )
  )
```

## Árvore de Decisão

Preparando a receita:

```{r}

tree_recipe <- recipe(medv ~ ., data = train) %>% 
  step_normalize(all_numeric(), -all_outcomes()) %>% 
  step_pca(c(where(is.numeric), -age, -all_outcomes()), num_comp = tune())

```

Definindo o modelo e o workflow:

```{r}

tree_fit <- decision_tree(
  tree_depth = 2,
  min_n = tune(),
  cost_complexity = tune()
) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")



tree_workflow <- workflow() %>% 
  add_model(tree_fit) %>% 
  add_recipe(tree_recipe)

```

Tunando os hiperparâmetros do modelo utilizando validação cruzada:

```{r}

doParallel::registerDoParallel(4)

tuned_tree <- tree_workflow %>% 
  tune_grid(
    resamples = cv_folds,
    metrics = metric_set(yardstick::mae)
  ) 


doParallel::stopImplicitCluster()

```

Visualizando os resultados *MAEs* obtidos para cada valor dos hiperparâmetros:

```{r}
autoplot(tuned_tree)
```

Selecionando os melhores valores, finalizando o modelo e guardando os resultados em nossa tibble:

```{r}
best_tree <- tuned_tree %>% 
  select_best("mae")


final_tree <- finalize_model(tree_fit, parameters = best_tree) %>% 
  fit(medv ~ ., train)
  

results <- results %>% 
    bind_rows(
    tibble(
      model = "ridge",
      mae = Metrics::mae(test$medv, predict(final_tree, test)$.pred)
    )
  )
```

## Random Forest

Realizando todo o processo repetido anteriormente para cada um dos modelos abaixo:


```{r}

doParallel::registerDoParallel(4)

rf_recipe <- recipe(medv ~ ., data = train) %>% 
  step_normalize(all_predictors(), -all_outcomes()) %>% 
  step_pca(c(where(is.numeric), -age, -all_outcomes()), num_comp = tune())

rf_fit <- rand_forest(mtry = 3, min_n = tune(), trees = 10) %>%
  set_engine("ranger", num.threads = 4, importance = "impurity") %>%
  set_mode("regression")

rf_workflow <- workflow() %>% 
  add_recipe(rf_recipe) %>% 
  add_model(rf_fit)

rf_tuned <- rf_workflow %>% 
  tune_grid(
    resamples = cv_folds,
    metrics = metric_set(yardstick::mae),
    control = control_grid(verbose = TRUE, save_pred = TRUE)
  )

doParallel::stopImplicitCluster()

best_rf <- rf_tuned %>% 
  select_best("mae")

rf_final <- rf_workflow %>% 
  finalize_workflow(best_rf) %>% 
  last_fit(splits)

results <- results %>% 
  bind_rows(
    tibble(
      model = 'rf',
      mae = Metrics::mae(rf_final$.predictions[[1]]$medv, rf_final$.predictions[[1]]$.pred)
    )
  )

results %>% 
  arrange(mae, ascending = FALSE)


```

O melhor modelo ajustado foi o Random Forest. Iremos visualizar os valores dos hiperparâmetros do modelo ajustado:

```{r}
best_rf
```

Como podemos ver, o número de componentes após a aplicação do PCA e realizando tunning nos valores dos hiperparâmetros, o melhor *MAE* obtido foi de um modelo que utiliza 3 componentes e *min_n* 4. Iremos visualizar a importância de cada um dos componentes utilizados:

```{r}
rf_final %>% 
  purrr::pluck(".workflow", 1) %>% # pega o modelo ajustado
  pull_workflow_fit() %>% # extrai o objeto do modelo
  vip::vip(num_features = 20) +
  theme_minimal(12)

```


