---
title: "Modelagem Preditiva Avançada"
subtitle: "Lista 1 - Diego Silva"
output:
  html_document:
    df_print: paged
---
<br />

> 1 - Apresente a análise da Atividade Integradora do trimestre passado usando o tidymodels. Não é necessário reproduzir todos os passos e ajustar todos os modelos, considere ajustar pelo menos 2 modelos preditivos fazendo a escolha de hiperparâmetros para avaliar o poder preditivo e decidir qual é o melhor modelo de acordo com uma métrica.
> 
> <br > 
>
> **Com objetivo de praticar a aplicação e avaliação dos modelos lecionados durante as aulas ministradas até o momento, iremos utilizar:**
>
> * *LASSO*
> * *KNN*
> * *Regressão Logística*;
> * *Árvore de Decisão*;
> * *Random Forest*;
> * *XGBoost*;

\n
\n

```{r, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE, warning = FALSE)
```

```{r, cache=TRUE, echo=FALSE}
library(tidymodels)
library(tidyverse)
library(gapminder)
library(rsample)
library(doParallel)
library(yardstick)
library(readr)
library(skimr)
```


## Lendo o conjunto de dados
```{r, reading-data, results="hide"}

df <- read_csv('data-treated.csv')

df$J1 <- as.factor(df$J1)

set.seed(123)
split <- initial_split(df, prop = .8, strata = "J1")

train <- training(split)
test <- testing(split)
```

## Criando os folds para validação cruzada que será utilizada posteriormente nos modelos

```{r, creating-cv-folds}

cv_folds <- vfold_cv(train, v = 10)

cv_folds
```

## Criando tabela para armazenar os resultados da análise

```{r, creating-results-tibble}
results <- tibble(model = NA_character_, threshold = NA_integer_,  accuracy = NA_integer_, fbeta = NA_integer_, recall = NA, precision = NA_integer_)
```

## Definindo função para calculo das métricas para cada valor de corte 

```{r, defining-gpft}
cortes <- seq(0.01, 0.99, 0.01)

get_predictions_from_threshold <- function(threshold, df_gpft, model) {
  results <- df_gpft %>% 
    mutate(threshold_preds = ifelse(df_gpft$.pred_1 >= threshold, 1, 0)) %>% 
    mutate(observed = as.numeric(as.character(observed)), threshold_preds = as.numeric(as.character(threshold_preds)))
           
 tibble(
   model = model,
   fbeta = Metrics::fbeta_score(results$observed, results$threshold_preds),
   acc = Metrics::accuracy(results$observed, results$threshold_preds),
   precision = Metrics::precision(results$observed, results$threshold_preds),
   recall = Metrics::recall(results$observed, results$threshold_preds),
   threshold = threshold
   )
}
```

## Aplicação dos modelos

> **Disclaimer*: A fins de prática, serão realizadas a receita e sua preparação em todas as etapas.*

<br />

### LASSO

> Preparando a receita para o lasso

```{r, preparing-lasso-recipe}
lasso_recipe <- recipe(J1 ~ ., data = train)

lasso_prep <- prep(lasso_recipe)

# mudar para bake com new_data = NULL
#lasso_juiced <- juice(lasso_prep)

lasso_juiced <- bake(lasso_prep, new_data = NULL)

lasso_baked <- bake(lasso_prep, new_data = test)
```


> Definição do Modelo

```{r, defining_lasso}
lasso <- logistic_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet") %>% 
  set_mode("classification")
```

> Visualizando a especificação 

```{r, checking_lasso_model}
lasso

```
> Realizando o tunning 

```{r, lasso_tuning}
tune_lasso <- tune_grid(
  lasso,
  lasso_recipe,
  resamples = cv_folds,
  grid = 50,
  metrics = metric_set(roc_auc, yardstick::accuracy, yardstick::specificity, yardstick::sensitivity, yardstick::f_meas)
  )
```

> Visualizando as métricas do tune_grid em relação a regularização

```{r}
autoplot(tune_lasso)
```

> Selecionando a quantidade de regularização pelos valores obtidos da curva roc e f1

```{r}
best_lasso <- tune_lasso %>% 
  select_best("roc_auc", "f_meas") 
```

> Ajustando o modelo final

```{r}

fitted_lasso <- finalize_model(lasso, parameters = best_lasso) %>% 
  fit(J1 ~ ., data=lasso_juiced)
```

> Obtendo os resultados e salvando-os para cada valor de corte 

```{r}
lasso_results <- fitted_lasso %>% 
  predict(new_data = lasso_baked, type = "prob") %>%
  mutate(observed = lasso_baked$J1, model = "lasso") 

results <- results %>% 
  bind_rows(map(cortes, get_predictions_from_threshold, df_gpft = lasso_results, model = "lasso")) %>% 
  arrange(desc(fbeta))
```

### Regressão logística

> Preparando a receita para a aplicação do modelo logístico

```{r, lr-recipe}

lr_recipe <- recipe(J1 ~ ., train) %>% # define a receita, com a variavel resposta e os dados de treinamento
  step_normalize(all_numeric())

lr_prep <- prep(lr_recipe)# prepara a receita definida acima

lr_juiced <- juice(lr_prep) # obtem os dados de treinamento processados

baked_test <- bake(lr_prep, new_data = test) # obtem os dados de teste processados

```


```{r, evaluating-lr}
lr_fit <- logistic_reg() %>% 
  set_engine("glm") %>% 
  fit(J1 ~ ., lr_juiced) %>% 
  set_mode("classification")


lr_results <- lr_fit %>% 
  predict(new_data = baked_test, type = "prob") %>% 
  mutate(observed = test$J1, modelo = "logistic regression") 

results <- results %>% 
  bind_rows(map(cortes, get_predictions_from_threshold, df_gpft = lr_results, model = 'lr'))
```

<br />

### Árvore de Decisão

<br /> 

> Para a Árvore de Decisão, iremos realizar o *tuning* dos parâmetros para avaliarmos o seu poder preditivo e aplicaremos validação cruzada para estimarmos o erro de classificação.

```{r, tree-recipe}
tree_recipe <- recipe(J1 ~ ., train) %>% 
  step_normalize(all_numeric())

tree_prep <- prep(tree_recipe)

tree_juiced <- bake(tree_prep, new_data = NULL)

tree_baked <- bake(tree_prep, new_data = test)

```

> Após a preparação da receita, iremos criar e ajustar a árvore de decisão tunada. 

<br />

> Adicionando paralelismo

```{r, adding-do-parallel}
doParallel::registerDoParallel()
```

<br />

> Especificando a árvore de decisão tunada

```{r, defining-decision-tree}

tree <- decision_tree(
  tree_depth = tune(), 
  cost_complexity = tune()) %>%
  set_engine("rpart") %>% 
  set_mode("classification") 

tree
```

> Tunando a árvore

```{r, tunning-tree}
tune_tree <- tune_grid(
  tree,
  tree_recipe,
  resamples = cv_folds,
  grid = 30,
  metrics = metric_set(roc_auc, yardstick::accuracy, yardstick::specificity, yardstick::sensitivity, yardstick::f_meas)
  )

tune_tree
```

> Analisando o gráfico contendo as métricas e os valores obtidos para cada um dos valores assumidos para os hiperparâmetros tunados:

```{r, autoplot-tree}
autoplot(tune_tree)
```

> Coletando as métricas

```{r, collecting-tree-metrics}

tune_tree %>% 
  collect_metrics() %>% 
  head(5)
```


> Selecionando os melhores valores para os hiperparâmetros pela curva roc e salvando-os para ajustarmos o modelo final com os valores encontrados.

> Visualizando os valores de hiperparâmetro que nos dá os melhores valores de roc_auc e specificity

```{r, visualizing-best-tree}
best_tree <-tune_tree %>% 
  select_best("roc_auc", "f_meas")
```

> Finalizando o modelo e obtendo as predictions para cada valor de corte. Adicionaremos também a tibble resultante a tibble dos nossos resultados.


```{r, finalizng-tree}
fit_tree <- finalize_model(tree, parameters = best_tree) %>% 
  fit(J1 ~ ., data=train)
 
fitted_tree <- fit_tree %>% 
  predict(new_data = test, type = "prob") %>% 
  mutate(observed = test$J1)

results <- results %>% 
  bind_rows(map(cortes, get_predictions_from_threshold, df_gpft = fitted_tree, model = 'tree'))
```

### Floresta Aleatória

> Assim como na Árvore de Decisão, também realizaremos o tunning dos hiperparâmetros e validação cruzada para estimarmos o erro de classificação.

> Começaremos pela receita

```{r}
rf_recipe <- recipe(J1 ~ ., data = train)

rf_prep <- prep(rf_recipe)

rf_baked_train <- bake(rf_prep, new_data = NULL)

rf_baked_test <- bake(rf_prep, new_data = test)
```

