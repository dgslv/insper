---
title: "Modelagem Preditiva Avançada"
subtitle: "Lista 1 - Diego Silva"
output:
  html_document:
    df_print: paged
---
<br />


# Exercício 1

> Apresente a análise da Atividade Integradora do trimestre passado usando o tidymodels. Não é necessário reproduzir todos os passos e ajustar todos os modelos, considere ajustar pelo menos 2 modelos preditivos fazendo a escolha de hiperparâmetros para avaliar o poder preditivo e decidir qual é o melhor modelo de acordo com uma métrica.
> 
> <br > 
>
> **Com objetivo de praticar a aplicação e avaliação dos modelos lecionados durante as aulas ministradas até o momento, iremos utilizar:**
>
> * *LASSO*
> * *KNN*
> * *Regressão Logística*;
> * *Árvore de Decisão*;
> * *Random Forest*;
> * *XGBoost*;

\n
\n

```{r, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE, warning = FALSE)
```

```{r, cache=TRUE, echo=FALSE, results = "hide"}
library(tidymodels)
library(tidyverse)
library(gapminder)
library(rsample)
library(doParallel)
library(yardstick)
library(readr)
library(skimr)
options(tidymodels.dark = TRUE)
```


## Lendo o conjunto de dados
```{r, reading-data, results="hide"}

df <- read_csv('data-treated.csv')

df$J1 <- as.factor(df$J1)

set.seed(123)
split <- initial_split(df, prop = .8, strata = "J1")

train <- training(split)
test <- testing(split)
```

## Criando os folds para validação cruzada que será utilizada posteriormente nos modelos

```{r, creating-cv-folds}

cv_folds <- vfold_cv(train, v = 10)

cv_folds
```

## Criando tabela para armazenar os resultados da análise

```{r, creating-results-tibble}
results <- tibble(model = NA_character_, threshold = NA_integer_,  accuracy = NA_integer_, fbeta = NA_integer_, recall = NA, precision = NA_integer_)
```

## Definindo função para calculo das métricas para cada valor de corte 

```{r, defining-gpft}
cortes <- seq(0.01, 0.99, 0.01)

get_predictions_from_threshold <- function(threshold, df_gpft, model) {
  results <- df_gpft %>% 
    mutate(threshold_preds = ifelse(df_gpft$.pred_1 >= threshold, 1, 0)) %>% 
    mutate(observed = as.numeric(as.character(observed)), threshold_preds = as.numeric(as.character(threshold_preds)))
           
 tibble(
   model = model,
   fbeta = Metrics::fbeta_score(results$observed, results$threshold_preds),
   acc = Metrics::accuracy(results$observed, results$threshold_preds),
   precision = Metrics::precision(results$observed, results$threshold_preds),
   recall = Metrics::recall(results$observed, results$threshold_preds),
   threshold = threshold
   )
}
```

## Aplicação dos modelos

> **Disclaimer*: A fins de prática, serão realizadas a receita e sua preparação em todas as etapas.*

<br />

### LASSO

> Preparando a receita para o lasso

```{r, preparing-lasso-recipe}
lasso_recipe <- recipe(J1 ~ ., data = train)

lasso_prep <- prep(lasso_recipe)

lasso_juiced <- bake(lasso_prep, new_data = NULL)

lasso_baked <- bake(lasso_prep, new_data = test)
```


> Definição do Modelo

```{r, defining_lasso}
lasso <- logistic_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet") %>% 
  set_mode("classification")
```

> Visualizando a especificação 

```{r, checking_lasso_model}
lasso
```
> Realizando o tunning 

```{r, lasso_tuning}
tune_lasso <- tune_grid(
  lasso,
  lasso_recipe,
  resamples = cv_folds,
  grid = 50,
  metrics = metric_set(roc_auc, yardstick::accuracy, yardstick::specificity, yardstick::sensitivity, yardstick::f_meas)
  )
```

> Visualizando as métricas do tune_grid em relação a regularização

```{r}
autoplot(tune_lasso)
```

> Selecionando a quantidade de regularização pelos valores obtidos da curva roc e f1

```{r}
best_lasso <- tune_lasso %>% 
  select_best("roc_auc", "f_meas") 
```

> Ajustando o modelo final

```{r}

fitted_lasso <- finalize_model(lasso, parameters = best_lasso) %>% 
  fit(J1 ~ ., data=lasso_juiced)
```

> Obtendo os resultados e salvando-os para cada valor de corte 

```{r}
lasso_results <- fitted_lasso %>% 
  predict(new_data = lasso_baked, type = "prob") %>%
  mutate(observed = lasso_baked$J1, model = "lasso") 

results <- results %>% 
  bind_rows(map(cortes, get_predictions_from_threshold, df_gpft = lasso_results, model = "lasso")) %>% 
  arrange(desc(fbeta))
```

### Regressão logística

> Preparando a receita para a aplicação do modelo logístico

```{r, lr-recipe}

lr_recipe <- recipe(J1 ~ ., train) %>% # define a receita, com a variavel resposta e os dados de treinamento
  step_normalize(all_numeric())

lr_prep <- prep(lr_recipe)# prepara a receita definida acima

lr_juiced <- juice(lr_prep) # obtem os dados de treinamento processados

baked_test <- bake(lr_prep, new_data = test) # obtem os dados de teste processados

```


```{r, evaluating-lr}
lr_fit <- logistic_reg() %>% 
  set_engine("glm") %>% 
  fit(J1 ~ ., lr_juiced) %>% 
  set_mode("classification")


lr_results <- lr_fit %>% 
  predict(new_data = baked_test, type = "prob") %>% 
  mutate(observed = test$J1, modelo = "logistic regression") 

results <- results %>% 
  bind_rows(map(cortes, get_predictions_from_threshold, df_gpft = lr_results, model = 'lr'))
```

<br />

### Árvore de Decisão

<br /> 

> Para a Árvore de Decisão, iremos realizar o *tuning* dos parâmetros para avaliarmos o seu poder preditivo e aplicaremos validação cruzada para estimarmos o erro de classificação.

```{r, tree-recipe}
tree_recipe <- recipe(J1 ~ ., train) %>% 
  step_normalize(all_numeric())

tree_prep <- prep(tree_recipe)

tree_juiced <- bake(tree_prep, new_data = NULL)

tree_baked <- bake(tree_prep, new_data = test)

```

> Após a preparação da receita, iremos criar e ajustar a árvore de decisão tunada. 

<br />

> Adicionando paralelismo

```{r, adding-do-parallel}
doParallel::registerDoParallel(cores = 4)
```

<br />

> Especificando a árvore de decisão tunada

```{r, defining-decision-tree}

tree <- decision_tree(
  tree_depth = tune(), 
  cost_complexity = tune()) %>%
  set_engine("rpart") %>% 
  set_mode("classification") 

tree
```

> Tunando a árvore

```{r, tunning-tree}

tune_tree <- tune_grid(
  tree,
  tree_recipe,
  resamples = cv_folds,
  grid = 30,
  metrics = metric_set(roc_auc, yardstick::accuracy, yardstick::specificity, yardstick::sensitivity, yardstick::f_meas)
  )

tune_tree
```

> Analisando o gráfico contendo as métricas e os valores obtidos para cada um dos valores assumidos para os hiperparâmetros tunados:

```{r, autoplot-tree}
autoplot(tune_tree)
```

> Coletando as métricas

```{r, collecting-tree-metrics}

tune_tree %>% 
  collect_metrics() %>% 
  head(5)
```


> Selecionando os melhores valores para os hiperparâmetros pela curva roc e salvando-os para ajustarmos o modelo final com os valores encontrados.

> Visualizando os valores de hiperparâmetro que nos dá os melhores valores de roc_auc e specificity

```{r, visualizing-best-tree}
best_tree <- tune_tree %>% 
  select_best("roc_auc", "f_meas")
```

> Finalizando o modelo e obtendo as predictions para cada valor de corte. Adicionaremos também a tibble resultante a tibble dos nossos resultados.


```{r, finalizng-tree}
fit_tree <- finalize_model(tree, parameters = best_tree) %>% 
  fit(J1 ~ ., data=train)
 
tree_preds <- fit_tree %>% 
  predict(new_data = test, type = "prob") %>% 
  mutate(observed = test$J1)

results <- results %>% 
  bind_rows(map(cortes, get_predictions_from_threshold, df_gpft = tree_preds, model = 'tree'))
```

### Floresta Aleatória

> Assim como na Árvore de Decisão, também realizaremos o tunning dos hiperparâmetros e validação cruzada para estimarmos o erro de classificação.

> Começaremos pela receita:

```{r}
rf_recipe <- recipe(J1 ~ ., data = train)

rf_prep <- prep(rf_recipe)

rf_baked_train <- bake(rf_prep, new_data = NULL)

rf_baked_test <- bake(rf_prep, new_data = test)
```

> Seguiremos agora para a especificação do modelo:

```{r}
rf <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

rf
```

> Tunando o modelo

```{r message=TRUE}

tune_rf <- tune_grid(
  rf,
  rf_recipe,
  resamples = cv_folds,
  grid = 2,
  control = control_grid(verbose = TRUE)
)

tune_rf
```

> Selecionando o melhor modelo pela área abaixo da curva e f1

```{r}
best_rf <- tune_rf %>% 
  select_best("roc_auc", "f_meas")
```


> Finalizando o modelo com os hiperparametros obtidos

```{r}
fit_rf <- finalize_model(rf, parameters = best_rf) %>% 
  fit(J1 ~ ., data = train)

```

> Calculando as métricas de interesse para cada valor de corte

```{r}

rf_preds <-  fit_rf %>%
  predict(new_data = test, type="prob")

results %>% 
  bind_rows(map(cortes, get_predictions_from_threshold, df_gpft = rf_preds, model = 'rf'))
```


```{r}
stopImplicitCluster()
```


```{r}
results
```


# Exercício 2

> Considere um modelo linear com todas as variáveis preditoras e um modelo linear com 5 componentes principais para previsão do valor mediano dos imóveis do banco de dados Boston (acesse com
MASS::Boston ). Para avaliar o desempenho dos modelos, faça uma divisão dos dados de 80% para treinamento e o restante para teste. Indique qual métrica está sendo utilizada para a comparação dos modelos e defina o melhor modelo preditivo. Dicas:
Note que queremos predizer o valor mediano dos imóveis, não médio. O que isso muda em nossa avaliação?
Para comparar o modelo sem PCA e com PCA, você pode usar tune() no número de componentes e, na tune_grid() adicionar os valores 0 e 4. O valor zero fará com que o PCA não seja ajustado, enquanto que o valor 4 considerará 4 componentes principais.


```{r, echo=FALSE}
library(MASS)
```


> Para a previsão do valor mediano dos imovéis, iremos minimizar o MAE (*Mean Absolute Error*), uma vez que isto indicaria uma boa previsibilidade de valores medianos. Em relação aos modelos que serão utilizados, segue a lista abaixo:
>
> * Regressão Linear
> * LASSO
> * Ridge
> * Árvore de Decisão
> * Floresta Aleatória
> * XGBoost

> Carregaremos o dataset e veremos a distribuição dos valores obtidos em cada uma das colunas

```{r}
df <- MASS::Boston
summary(df)

```

> Separando os dados em treino e teste, e então criando os *cross-validation folds*

```{r}
splits <- initial_split(df, prop = .8, strata = "medv")

train <- training(splits)
test <- testing(splits)

cv_folds <- vfold_cv(train, v = 10)
```

> Definindo tibble para armazenar os resultados obtidos por cada modelo em relação a métrica de interesse:

```{r}
results <- tibble(model = NA, mae = NA_integer_)
```

## Regressão Linear

> Preparando a receita:

```{r}
lm_recipe <- recipe(medv ~ ., data = train) %>% 
  step_normalize(all_numeric(), -all_outcomes())

lm_prep <- prep(lm_recipe)

train_lm <- bake(lm_prep, new_data = NULL)
test_lm <- bake(lm_prep, new_data = test)
  
```


> Definindo função para cálculo do *MAE* de cada fold da volidação cruzada

```{r}
get_maes <- function(split, model) {
  train_split <- training(split)
  test_split <- testing(split)
  
  fit <- model %>% 
    fit(medv ~ ., data = train_split)
  
  tibble(
    mae = Metrics::mse(test_split$medv, predict(fit, test_split)$.pred)
  )
}

```


> Estimando o erro de *MAE* do modelo linear com validação cruzada:

```{r}
lm_fit <- linear_reg() %>% 
  set_engine("lm")

cv_folds %>% 
  mutate(maes = map(lm_folds$splits, get_mae_from_lm, model = lm_fit)) %>% 
  unnest(maes) %>% 
  summarise_if(is.numeric, mean)
```
> Ajustando o modelo linear

```{r}

ajusted_lm <- lm_fit %>% 
  fit(medv ~ ., data = train_lm)

```

> Obtendo as previsões e o *MAE* do modelo linear ajustado e salvando-as no tibble de resultados

```{r}

results <- results %>% 
  bind_rows(tibble(model = "lm", mae = Metrics::mae(test_lm$medv, predict(ajusted_lm, test_lm)$.pred)))

```
## LASSO

> Preparando a receita para o modelo LASSO:

```{r}

lasso_recipe <- recipe(medv ~ ., data = train) %>% 
  step_normalize(all_numeric(), -all_outcomes()) 

lasso_prep <- prep(lasso_recipe)

lasso_train <- bake(lasso_prep, new_data = NULL)

lasso_test <- bake(lasso_prep, new_data = test)

```

> Defininção do modelo:

```{r}

lasso_fit <- linear_reg(
  penalty = tune(),
  mixture = 1
) %>% 
  set_engine("glmnet") %>% 
  set_mode("regression") 

```

> Tunando a regularização utilizando validação cruzada:

```{r}

lasso_tuned <- tune_grid(
  lasso_fit,
  lasso_recipe,
  resamples = cv_folds,
  grid = 10,
  metrics = metric_set(yardstick::mae)
)

```

> Visualizando o gráfico com os resultados:

```{r}
autoplot(lasso_tuned)
```

> Selecionando a regularização pelo melhor valor obtido de *MAE*:

```{r}
best_lasso <- lasso_tuned %>% 
  select_best("mae")

best_lasso
```

> Finalizando o modelo lasso:

```{r}
final_lasso <- finalize_model(lasso_fit, parameters = best_lasso) %>% 
  fit(medv ~ ., data = train)
```


> Estimando o *MAE* do modelo lasso ajustado e salvado-o em nosso *tibble* de resultados:


```{r}
results <- results %>% 
  bind_rows(tibble(model = "lasso", mae = Metrics::mae(test$medv, predict(final_lasso, lasso_test)$.pred)))

```

## Ridge

> Preparando a receita:

```{r}

ridge_recipe <- recipe(medv ~ ., data = train) %>% 
    step_normalize(all_numeric(), -all_outcomes())

ridge_prep <- prep(ridge_recipe)

ridge_train <- bake(ridge_prep, new_data = NULL)

ridge_test <- bake(ridge_prep, new_data = test)

```

> Definindo o modelo:

```{r}

ridge_fit <- linear_reg(
  penalty = tune(),
  mixture = 0
  ) %>% 
  set_engine("glmnet") %>% 
  set_mode("regression")

```

> Realizando validação cruzada:

```{r}

ridge_tuned <- tune_grid(
  ridge_fit,
  ridge_recipe,
  resamples = cv_folds,
  metrics = metric_set(yardstick::mae)
)

```

> Visualizando o gráfico do *MAE* em relação a regularização no modelo ridge:

```{r}
autoplot(ridge_tuned)
```

> Como podemos ver, os valores diferentes de regularização não tiveram *MAEs* diferentes.
> Finalizaremos o modelo, registrando o *MAE* do modelo ajustado abaixo:

```{r}
best_ridge <- ridge_tuned %>% 
  select_best()
  
final_ridge <- finalize_model(ridge_fit, parameters = best_ridge) %>% 
  fit(medv ~ ., data = train)

results <- results %>% 
  bind_rows(
    tibble(
      model = "ridge",
      mae = Metrics::mae(test$medv, predict(final_ridge, test)$.pred)
    )
  )
```

```{r}
results
```

# Árvore de Decisão

> Preparando a receita:

```{r}

tree_recipe <- recipe(medv ~ ., data = train) %>% 
  step_normalize(all_numeric(), -all_outcomes())

tree_prep <- prep(tree_recipe)

tree_train <- bake(tree_prep, new_data = NULL)

tree_test <- bake(tree_prep, new_data = test)

```

> Definindo o modelo:

```{r}

tree_fit <- decision_tree(
  tree_depth = tune(),
  min_n = tune(),
  cost_complexity = tune()
) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")

```

> Tunando os hiperparâmetros do modelo utilizando validação cruzada:

```{r}

tuned_tree <- tune_grid(
  tree_fit,
  tree_recipe,
  resamples = cv_folds,
  metrics = metric_set(yardstick::mae)
) 

```

> 

```{r}

```

